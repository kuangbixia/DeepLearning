# 细化[FCN](https://arxiv.org/abs/1411.4038)学习

## 1 FCN结构

### (1) VGG16结构

<img src="../Figures/VGG16.jpg" style="zoom:67%;" />

- 前部分的卷积层（包含ReLU）和池化层

  - 最大池化层：使用2x2的池化核，步长为2，使得图像缩小一半
  - 卷积层：使用3x3的卷积核，步长为1，填充设为1，使得经过卷积层时输出图像和输入图像保持相同的高和宽
    - 使用多个卷积层，小卷积核，减少参数
    - 如下面的代码段中cfg，分别是两个输出通道为64 ->（经过最大池化层）-> 两个输出通道为128 ->（经过最大池化层）-> 三个输出通道为256 ->（经过最大池化层）-> 三个输出通道为512 ->（经过最大池化层）-> 再接着三个输出通道为512的卷积层

  ```python
  cfg = {
      # ...
      'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
      # ...
  }
  
  def make_layers(cfg, batch_norm=False):
      layers = []
      in_channels = 3
      for v in cfg:
          if v == 'M':
              layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
          else:
              conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
              if batch_norm:
                  layers += (conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True))
              else:
                  layers += [conv2d, nn.ReLU(inplace=True)]
              in_channels = v
      return nn.Sequential(*layers)
  ```

- 后部分的全连接层（分类器）

  - 先经过一个平均池化层，将图像输出为7x7的大小
    - VGG16要求输出图像大小固定为224x224的RGB图像（3通道）
    - 通过上面所述的卷积层和池化层后，在最后一个池化层输出图像刚好是7x7的大小
  - 最后是三个全连接层

  ```python
  self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
  self.classifier = nn.Sequential(
      nn.Linear(512 * 7 * 7, 4096),
      nn.ReLU(True),
      nn.Dropout(),
      nn.Linear(4096, 4096),
      nn.ReLU(True),
      nn.Dropout(),
      nn.Linear(4096, num_classes)
  )
  ```

### (2) 从VGG16到FCN

![](../Figures/FCN_figure3_notes.JPG)

- 沿用了VGG16的前部分——卷积层和池化层

  - 卷积层：如上图conv1-conv5
  - 池化层：如上图pool1-pool5

- 将全连接层替换为卷积层

  - pool5后的卷积层（512通道 -> nclass通道，不改变图像大小）——_FCNHead()

    - 第一个卷积层：卷积核为3x3，步长为1，填充设为1，不改变输入图像的宽和高，且不改变通道数量
    - 第二个卷积层：卷积核为1x1，步长为1，无填充，不改变输入图像的宽和高，输出通道数量为类别数量

    ```python
    class _FCNHead(nn.Module):
        def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d, **kwargs):
            super(_FCNHead, self).__init__()
            inter_channels = in_channels // 512
            self.block = nn.Sequential(
                nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),
                norm_layer(inter_channels),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Conv2d(inter_channels, channels, 1)
            )
    ```

  - pool4后的卷积层（512通道 -> nclass通道，不改变图像大小）——score_pool4()

    ```python
    self.score_pool4 = nn.Conv2d(512, nclass, 1)
    ```

  - pool3后的卷积层（256通道 -> nclass，不改变图像大小）——score_pool3()

    ```python
    self.score_pool3 = nn.Conv2d(256, nclass, 1)
    ```

- 层融合

  - 通过反卷积（向上取样）放大图像

    - 使用双线性插值
    - 使得来自不同层的两个图像大小一致，进行融合 / 最后输出原图像大小的图像

  - 从后往前逐层融合

    - fcn32s

      <img src="..\Figures\FCN32s_train.JPG" style="zoom:67%;" />

    - fcn16s

      <img src="..\Figures\FCN16s_train.JPG" style="zoom:67%;" />

    - fcn8s

      <img src="E:\DeepLearning\Figures\FCN8s_train.JPG" style="zoom:67%;" />

- FCN vs VGG16
  - VGG16在经过一系列的卷积层（和池化层）后，输出512通道的图像，需要经过全连接层（分类器），产生一维（nclass大小）的输出
    - 分类器需要考虑到输入图像的每一个像素，产生分类输出
    - 因此，VGG16要求固定大小图像的输入，来确定到达全连接层的图像大小（像素个数），确定全连接的输入参数
    - VGG16要求输入224x224的RGB图像，经过五个池化层后，缩小到7x7（512通道，通道数量由卷积层决定），所以分类器的输入是512x7x7（固定的值）
  - FCN将全连接层替换为卷积层后，可以接受任意大小的图像的输入，通过反卷积可以产生相应大小的输出
    - FCN由各个卷积层和池化层构成
      - 卷积层通过设置3x3的卷积核，步长为1，填充为1，保证输出图像不改变大小，只关心输入输出通道（和图像大小无关）
      - 每个池化层只对输入图像缩小一半，对任意大小的输入图像都可用
    - 只要在最后反卷积时对图像依次放大一定倍数，即可恢复到原图像的大小，不需要考虑输入图像的大小

## 2 loss函数



## 3 量化评估指标

### (1) IoU/IU 交并比(Intersection over Union)

$$
IoU=\frac{target \cap prediction}{target \cup prediction}
$$

- 基于类进行计算，将每一类的IoU计算后，累加计算平均值 -> mean IoU均交并比

  - $$
    MIoU=\frac{1}{k+1}\sum_{i=0}^k\frac{n_{ii}}{\sum_{j=0}^kn_{ij}+\sum_{j=0}^kn_{ji}-n_{ii}}
    $$

  - $$
    其中，n_{ii}表示target中类别为i的像素预测为类别i的像素的个数\\
    \sum_{j=0}^kn_{ij}相当于target中类别为i的面积（单个类别的area\_lab），\\
    \sum_{j=0}^kn_{ji}相当于prediction中类别为i的面积（单个类别的area\_pred），\\
    n_{ii}相当于它们相交的面积（单个类别的area\_inter）
    $$

  - ```python
    def batch_intersection_union(output, target, nclass):
        """mIoU"""
        # inputs are numpy array, output 4D, target 3D
        mini = 1
        maxi = nclass
        nbins = nclass
        predict = torch.argmax(output, 1) + 1
        target = target.float() + 1
    
        predict = predict.float() * (target > 0).float()
        intersection = predict * (predict == target).float()
        # areas of intersection and union
        # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.
        area_inter = torch.histc(intersection.cpu(), bins=nbins, min=mini, max=maxi)
        area_pred = torch.histc(predict.cpu(), bins=nbins, min=mini, max=maxi)
        area_lab = torch.histc(target.cpu(), bins=nbins, min=mini, max=maxi)
        area_union = area_pred + area_lab - area_inter
        assert torch.sum(area_inter > area_union).item() == 0, "Intersection area should be smaller than Union area"
        return area_inter.float(), area_union.float()
    
    # 返回了统计了每个类别的像素的个数的Tensor
    inter, union = batch_intersection_union(pred, label, self.nclass)
    self.total_inter += inter
    self.total_union += union
    
    IoU = 1.0 * self.total_inter / (2.220446049250313e-16 + self.total_union)
    mIoU = IoU.mean().item()
    ```

    

### (2) pixcal accuracy 像素精度(PA)

$$
PA=\frac{\sum_{i=0}^{k}n_{ii}}{\sum_{i=0}^k \sum_{j=0}^kn_{ij}}\\
其中，n_{ii}表示target中类别为i的像素预测为类别i的像素的个数（correct）\\
\sum_{j=0}^kn_{ij}相当于target中类别为i的像素个数（labeled）
$$

```python
def batch_pix_accuracy(output, target):
    """PixAcc"""
    # inputs are numpy array, output 4D, target 3D
    predict = torch.argmax(output.long(), 1) + 1
    target = target.long() + 1

    pixel_labeled = torch.sum(target > 0).item()
    pixel_correct = torch.sum((predict == target) * (target > 0)).item()
    assert pixel_correct <= pixel_labeled, "Correct area should be smaller than Labeled"
    return pixel_correct, pixel_labeled

# 返回像素个数
correct, labeled = batch_pix_accuracy(pred, label)

self.total_correct += correct
self.total_label += labeled

pixAcc = 1.0 * self.total_correct / (2.220446049250313e-16 + self.total_label)  # remove np.spacing(1)
```



# 进一步实验

​	上周分别对FCN32s，FCN16s和FCN8s，在PASCAL VOC2012的数据集上做实验，得到输出效果图的直观上的对比情况。

## 1 训练

​	为了和github上的结果更好的对比，本次实验训练时也设置了60个epochs。

### (1) FCN32s

- loss曲线

![](..\Figures\loss_fcn32s.JPG)

- metric变化曲线

  - mean IoU

  ![](..\Figures\mIoU_FCN32s.JPG)

  - pixel accuracy

  ![](..\Figures\pixAcc_fcn32s.JPG)

### (2) FCN16s

- loss曲线

![](..\Figures\loss_fcn16s.JPG)

- 评估指标变化曲线

  - mean IoU

  ![](..\Figures\mIoU_FCN16s.JPG)

  - pixel accuracy

  ![](..\Figures\pixAcc_fcn16s.JPG)

### (3) FCN8s

- loss曲线

![](..\Figures\loss_fcn8s.JPG)

- 评估指标变化曲线

  - mean IoU

  ![](..\Figures\mIoU_FCN8s.JPG)

  - pixel accuracy

  ![](..\Figures\pixAcc_fcn8s.JPG)

## 2 测试

- FCN论文

| Methods | Backbone | DataSet | epochs | lr   | Mean IoU | pixAcc |
| ------- | -------- | ------- | ------ | ---- | -------- | ------ |
| FCN32s  | vgg16    | VOC2011 | ？     | ？   | 59.40    | 89.10  |
| FCN16s  | vgg16    | VOC2011 | ？     | ？   | 62.40    | 90.00  |
| FCN8s   | vgg16    | VOC2011 | ？     | ？   | 62.70    | 90.30  |

- [github](https://github.com/Tramac/awesome-semantic-segmentation-pytorch)

| Methods | Backbone | DataSet | epochs | lr     | Mean IoU | pixAcc |
| ------- | -------- | ------- | ------ | ------ | -------- | ------ |
| FCN32s  | vgg16    | VOC2012 | 60     | 0.0001 | 47.50    | 85.39  |
| FCN16s  | vgg16    | VOC2012 | 60     | 0.0001 | 49.16    | 85.98  |
| FCN8s   | vgg16    | VOC2012 | 60     | 0.0001 | 48.87    | 85.02  |

- 我的实验

| Methods | Backbone | DataSet | epochs | lr     | Mean IoU | pixAcc |
| ------- | -------- | ------- | ------ | ------ | -------- | ------ |
| FCN32s  | vgg16    | VOC2012 | 60     | 0.0001 | 46.02    | 85.88  |
| FCN16s  | vgg16    | VOC2012 | 60     | 0.0001 | 47.74    | 87.17  |
| FCN8s   | vgg16    | VOC2012 | 60     | 0.0001 | 47.38    | 87.12  |

| Methods | Backbone | DataSet | epochs | lr     | Mean IoU | pixAcc |
| ------- | -------- | ------- | ------ | ------ | -------- | ------ |
| FCN32s  | vgg16    | VOC2012 | 200    | 0.0001 | 45.77    | 85.84  |
| FCN16s  | vgg16    | VOC2012 | 200    | 0.0001 | 48.57    | 87.26  |
| FCN8s   | vgg16    | VOC2012 | 200    | 0.0001 | 48.57    | 87.33  |

