# 关于卷积网络中的maps

## 1 feature maps 特征图

- 对于卷积层的输入输出，数据是三维的。(channel, height, width)
- 因此，灰度图像的每次输入会产生一个feature map（只有一个通道），彩色图像的每次输入会产生三个feature map（三个通道）

## 2 belief maps 置信图

​	CNN网络中经过全连接层后，最终输出的特征图，通道数为类别个数



# 学习论文[DeepLabv3:Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)

- 作者（2017，部分同DeepLabv2）
  - Liang-Chieh Chen
  - George Papandreou
  - Florian Schroff
  - Hartwig Adam



## 摘要&介绍

### 1 主要内容

#### (1) 重温空洞卷积（用处）

- 调整过滤器的接受野
- 控制DCNN输出特征图的分辨率

- ASPP解决多尺度图像问题：
  - 采用多个空洞卷积层并行处理

#### (2) 改进ASPP模块

- 用编码了全局上下文的特征图

#### (3) 丢弃CRF的后处理

### 2 DCNN应用到语义分割的两个挑战

#### (1) 由池化和卷积操作导致的特征图分辨率下降

- 具体的空间信息逐渐丢失
- 解决：
  - 去掉最后两个下采样层
  - **空洞卷积**：对其后的卷积层的过滤器上采样（不增加额外的参数），以更大的接受野，处理分辨率更高的特征图

#### (2) 不同尺度图像的输入

- 四种方法（如下图）
  
  <img src=".\Figures\DeepLabv3\four_methods_for_multiscale_images.JPG" style="zoom:67%;" />
  
  1. 图像金字塔
     - 对图像进行不同比例的缩放，分别提取特征后整合
  2. Encoder-Decoder 编码器-解码器结构
     - FCN中采用的方法
     - 对不同层的特征图进行上采样并融合，输出和原图像大小一样的特征图
  3. 空洞卷积
     - DeepLabv1中采用的方法
     - 将一个两层MLP添加到输入图像和pool1、pool2、pool3、pool4的输出
       - MLP（两层）：
         1. 128个3x3的卷积过滤器
         2. 128个1x1的卷积过滤器
     - 将MLP输出的特征图（5个）和主网络的最后一层输出的特征图结合
  4. ASPP:Atrous Spatial Pyramid Pooling 空洞空间金字塔
     - DeepLabv2中采用的方法
     - 不同的池化操作具有不同的采样率和接受野
     - **缺点：**
       - 当给3x3的卷积核设置一个很大的采样率时，会因为**图像边界效应**，导致无法捕获远程信息
     - **解决：**
       - 将3x3卷积退化成1x1卷积
       - 并将图像级别的特征加入到ASPP模块中

## 相关工作

​	全局特征（全局上下文交互）对语义分割中的像素分类很重要，以下是四种利用图像全局上下文信息进行语义分割的FCN网络。

### 1 Image Pyramid 图像金字塔

#### (1) 结构

- 使用同一个模型（共享同样的参数权重），构造适应不同尺度的输入图像的分支（如上图的子图a）
- 大尺度的分支（如上图的子图a左）输出的特征保留了小目标的细节信息，小尺度的分支（如上图的子图a右）输出的特征编码了远程（全局）的上下文
- 对输入的原图像进行不同比例的缩放，输入到不同的分支中，最后将各个分支输出的特征图（大小不同）整合

#### (2) 缺点

​	由于GPU内存的限制，不适用于更大型的（更深度的）DCNN。

### 2 Encoder-Decoder 编码器-解码器

#### (1) 结构

​	由两部分组成

1. encoder 编码器
   - 特征图的空间维度（大小）逐渐减小（如上图的子图b左）
   - 因此，编码器的更深层的输出就更容易捕获到更远程（全局）的信息
2. decoder 解码器
   - 图像的细节和空间维度（大小）逐渐恢复（如上图的子图b右）

### 3 Atrous Convolution 空洞卷积

​	很多基于空洞卷积的模型已经被应用到语义分割中。

### 4 Spatial Pyramid Pooling 空间金字塔池化

#### (0) Context Module 上下文模块

1. 作用

   ​	级联一些额外的模块，用于编码远程（全局）的上下文

2. 三种方法

   1. 将DenseCRF（具有高维滤波算法）结合到DCNN
   2. 结合CRF和DCNN：在DCNN的最高层的特征图（有**nClass**个通道的**belief map**）后增加几个额外的卷积层，来捕获上下文信息
   3. 学习一个全局的稀疏的高维卷积，并结合高斯CRF和DCNN

#### (1) ASPP结构（DeepLabv2）

​	通过并行的多个具有不同的采样率的空洞卷积层，来捕获不同尺度的信息（如上图的子图d）

#### (2) PSP:Pyramid Scene Parsing Net 金字塔场景解析网络

​	在多个网格尺度上进行空间池化

#### (3) 把空洞卷积作为SPP的上下文模块/工具（DeepLabv3）

​	提出一个通用框架，可以应用到任何网络

- 级联模块结构
  - 对**ResNet**原网络中最后一个块（block），复制多个副本，并将它们级联（串联）起来
  - 重温包含了并行的多个空洞卷积层的**ASPP**模块
- 关于以上的级联模块的输入，**直接应用到特征图**上而不是置信图
- 实验证明，进行**批量标准化训练**很重要
- 为了更好的捕获全局上下文，用**图像级别的特征**来增强ASPP

## 方法

### 1 空洞卷积——用于密集特征提取（同DeepLabv2）

- 已经证明，将DCNN转换成全卷积的形式，对语义分割很有效。
- 重复的最大池化层逐渐降低了输出特征图的分辨率（如，FCN中就在高度和宽度上都缩小到原图像的1/32，使用反卷积恢复分辨率）

#### (1) 结构（如下图）

<img src=".\Figures\DeepLabv3\atrous_convolution.JPG" style="zoom:67%;" />

- 表达式（同DeepLabv2）
  $$
  y[i]=\sum_{k=1}^Kx[i+r\cdot k]w[k]\\
  其中，r为步长
  $$

#### (2) 作用

- 标准的卷积中，r=1；空洞卷积中，**r（dilation_rate 扩张率/输入步长）**可以设置不同的值，来**修改过滤器的接受野大小**
- 空洞卷积可以通过设置output_stride来**恢复图像大小**
  - 如，DCNN输出的特征图是原图像的1/32，可以通过设置output_stride=32来恢复分辨率
- 空洞卷积可以**控制输出特征密集程度**（如下，增大特征密度），且不增加额外的参数
  - 如果想要输出的特征更密集（增大两倍），则将最后的最大池化（和卷积）的stride减为1（则不在上一层的1/16输出再减半），则之后的**卷积层的输入步长r增大为2**（由于输入的图像的分辨率增大了，由1/32增大到了1/16），同时设置output_stride=16恢复到原图像大小

### 2 更深入空洞卷积